# -*- coding: utf-8 -*-
"""
Created on Fri Nov 26 02:54:32 2021

@author: gabriel
"""

# -*- coding: utf-8 -*-
"""Mineração em Redes Heterogêneas - Exemplo com Propagação de Embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q4Lrt_Pa36r_ygwZYq6vR1XxlW8t_2Rq

# Carregando o Dataset
"""

# !git clone https://github.com/karansikka1/documentIntent_emnlp19/



!tar -xvf documentIntent_emnlp19/resnet18_feat.tar

import pandas as pd

df_data_train = pd.read_json('documentIntent_emnlp19/splits/train_split_0.json')
df_data_train

df_data_test = pd.read_json('documentIntent_emnlp19/splits/val_split_0.json')
df_data_test

"""# Extraindo Características Visuais"""

import numpy as np

L_img_features = []
for index,row in df_data_train.iterrows():
  features = np.load('resnet18_feat/'+row['id']+'.npy')
  L_img_features.append(features)

df_data_train['img_features'] = L_img_features
df_data_train

import numpy as np

L_img_features = []
for index,row in df_data_test.iterrows():
  features = np.load('resnet18_feat/'+row['id']+'.npy')
  L_img_features.append(features)

df_data_test['img_features'] = L_img_features
df_data_test

"""## Gerador de nós da rede heterogênea a partir das características visuais"""

import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
X = np.array( df_data_train['img_features'].to_list() )
y = np.array( df_data_train['intent'].to_list() )
from sklearn.svm import SVC
clf_img = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf_img.fit(X, y)

X_test = np.array( df_data_test['img_features'].to_list() )
y_test = np.array( df_data_test['intent'].to_list() )
clf_img.score(X_test,y_test)

"""# Extraindo Características Textuais"""

!pip install ktrain
import keras
import ktrain
from ktrain import text
import os

os.environ["CUDA_VISIBLE_DEVICES"]="0";

"""## Gerador de nós da rede heterogênea a partir das características textuais"""

MODEL_NAME = 'bert-base-uncased'
class_names = list(df_data_train['intent'].unique())
class_names.sort()
maxlen = 128

x_train = df_data_train['caption'].to_list()
y_train = df_data_train['intent'].to_list()

x_test = df_data_test['caption'].to_list()
y_test = df_data_test['intent'].to_list()

t = text.Transformer(MODEL_NAME, maxlen=maxlen)
trn = t.preprocess_train(x_train, y_train)
val = t.preprocess_test(x_test, y_test)
model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)
learner.fit_onecycle(5e-5, 3)

preds = learner.predict(trn)
L_preds = []
for label_cod in np.argmax(preds,axis=1):
  L_preds.append( class_names[label_cod] )

df_data_train['bert_label'] = L_preds
df_data_train

preds = learner.predict(val)
L_preds = []
for label_cod in np.argmax(preds,axis=1):
  L_preds.append( class_names[label_cod])

df_data_test['bert_label'] = L_preds
df_data_test

"""# Gerando embeddings iniciais (a partir do BERT)"""

L = []
from tqdm.notebook import tqdm
from transformers import *
import tensorflow as tf
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)



for index,row in tqdm(df_data_train.iterrows(),total=len(df_data_train)):

  input_ids = tf.constant(tokenizer.encode(row['caption']))[None, :]  # Batch size 1
  outputs = model.layers[0](input_ids)
  last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
  v = np.average(last_hidden_states.numpy()[0],axis=0)
  L.append(v)

df_data_train['text_features'] = L
df_data_train

L = []
for index,row in tqdm(df_data_test.iterrows(),total=len(df_data_test)):

  input_ids = tf.constant(tokenizer.encode(row['caption']))[None, :]  # Batch size 1
  outputs = model.layers[0](input_ids)
  last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
  v = np.average(last_hidden_states.numpy()[0],axis=0)
  L.append(v)

df_data_test['text_features'] = L

df_data_test

"""# Framework de Regularização

## Construção da Rede Heterogênea
"""

import networkx as nx

G = nx.Graph()


for index,row in df_data_train.iterrows(): # nós gerados por visual features (treino)
  img_node = clf_img.predict( [row['img_features']] )
  img_node = img_node[0]+':img_features'

  txt_node = row['bert_label']+':txt_features'
  
  G.add_edge(row['id']+':obj', img_node)
  G.add_edge(row['id']+':obj', txt_node)



for edge in G.edges():
  print(edge)

for index,row in df_data_test.iterrows(): # nós gerados por visual features (teste)
  img_node = clf_img.predict( [row['img_features']] )
  img_node = img_node[0]+':img_features'

  txt_node = row['bert_label']+':txt_features'
  
  G.add_edge(row['id']+':obj', img_node)
  G.add_edge(row['id']+':obj', txt_node)

for edge in G.edges():
  print(edge)

"""## Adicionando Embeddings em alguns nós da rede"""

for index,row in tqdm(df_data_train.iterrows(),total=len(df_data_train)):
  node_id = row['id']
  node_emb_txt = row['text_features']
  G.nodes[str(node_id)+':obj']['y'] = node_emb_txt

for index,row in tqdm(df_data_test.iterrows(),total=len(df_data_test)):
  node_id = row['id']
  node_emb_txt = row['text_features']
  G.nodes[str(node_id)+':obj']['y'] = node_emb_txt

import random
from tqdm.notebook import tqdm
def regularization(G, dim, iterations=15,mi=0.85):

  nodes = []

  # inicializando vetor f para todos os nodes
  for node in G.nodes():
    G.nodes[node]['f'] = np.array([0.0]*dim)
    if 'y' in G.nodes[node]: G.nodes[node]['f'] = G.nodes[node]['y']*1.0
    nodes.append(node)

  pbar = tqdm(range(0,iterations))

  for iteration in pbar:
    random.shuffle(nodes)
    energy = 0.0

    # percorrendo cada node
    for node in nodes:
      f_new = np.array([0.0]*dim)
      f_old = np.array(G.nodes[node]['f'])*1.0
      sum_w = 0.0

      # percorrendo vizinhos do onde
      for neighbor in G.neighbors(node):
          w = 1.0
          if 'weight' in G[node][neighbor]: w = G[node][neighbor]['weight']

          w /= np.sqrt(G.degree[neighbor])

          f_new += w*G.nodes[neighbor]['f']

          sum_w += w

      f_new /= sum_w
      

      G.nodes[node]['f'] = f_new*1.0

      if 'y' in G.nodes[node]:
        G.nodes[node]['f'] = G.nodes[node]['y']*mi + G.nodes[node]['f']*(1.0-mi)
      
      energy += np.linalg.norm(f_new-f_old)

    iteration+=1
    message = 'Iteration '+str(iteration)+' | Energy = '+str(energy)
    pbar.set_description(message)

  return G

"""# Propagando as Embeddings"""

regularization(G, 768, iterations=30, mi=0.7)

"""# Avaliando"""

X_train = []
Y_train = []
for index,row in df_data_train.iterrows():
  X_train.append( G.nodes[str(row['id'])+':obj']['f'] )
  Y_train.append(row['intent'])

X_test = []
Y_test = []
for index,row in df_data_test.iterrows():
  X_test.append( G.nodes[str(row['id'])+':obj']['f'] )
  Y_test.append(row['intent'])

import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
X = np.array( X_train )
y = np.array( Y_train )
from sklearn.svm import SVC
clf_img = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf_img.fit(X, y)

X_test = np.array( X_test )
y_test = np.array( Y_test )
clf_img.score(X_test,y_test)